{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 178\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Backward passes and optimizer steps\u001b[39;00m\n\u001b[0;32m    177\u001b[0m text_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 178\u001b[0m \u001b[43mtext_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m image_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    181\u001b[0m image_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Aman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Aman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:218\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;129m@_use_grad_for_differentiable\u001b[39m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m            and returns the loss.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_graph_capture_health_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Aman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:436\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_cuda_graph_capture_health_check\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;66;03m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling()\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_built()\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[0;32m    435\u001b[0m     ):\n\u001b[1;32m--> 436\u001b[0m         capturing \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_current_stream_capturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m capturing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    439\u001b[0m             group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups\n\u001b[0;32m    440\u001b[0m         ):\n\u001b[0;32m    441\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    442\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    443\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    444\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but param_groups\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m capturable is False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\Aman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\graphs.py:30\u001b[0m, in \u001b[0;36mis_current_stream_capturing\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_current_stream_capturing\u001b[39m():\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cuda_isCurrentStreamCapturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Custom Dataset definition\n",
    "class CrisisDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, feature_extractor, transform=None):\n",
    "        self.df = dataframe.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transform = transform\n",
    "        # Map string labels to binary values (informative: 1, not_informative: 0)\n",
    "        self.df['label_image_bin'] = self.df['label_image'].map({'informative': 1, 'not_informative': 0})\n",
    "        self.df['label_bin'] = self.df['label'].map({'informative': 1, 'not_informative': 0})\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        tweet_text = row['tweet_text']\n",
    "        image_path = row['image']\n",
    "        \n",
    "        # Load image and convert to RGB\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = self.feature_extractor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "            \n",
    "        # Tokenize tweet text\n",
    "        encoding = self.tokenizer(tweet_text, padding=\"max_length\", truncation=True, \n",
    "                                  max_length=128, return_tensors=\"pt\")\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Convert labels to tensors (as floats for BCE loss)\n",
    "        label_image = torch.tensor(row['label_image_bin'], dtype=torch.float)\n",
    "        label = torch.tensor(row['label_bin'], dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'image': image,\n",
    "            'label_image': label_image,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# Text Classifier using BERT\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased'):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # [batch_size, hidden_size]\n",
    "        logits = self.classifier(pooled_output)  # [batch_size, 1]\n",
    "        return logits\n",
    "\n",
    "# Image Classifier using ViT\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, vit_model_name='google/vit-base-patch16-224-in21k'):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        outputs = self.vit(pixel_values=images)\n",
    "        cls_output = outputs.last_hidden_state[:, 0]  # [batch_size, hidden_size]\n",
    "        logits = self.classifier(cls_output)          # [batch_size, 1]\n",
    "        return logits\n",
    "\n",
    "# Multimodal Classifier combining BERT and ViT\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', vit_model_name='google/vit-base-patch16-224-in21k'):\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        combined_size = self.bert.config.hidden_size + self.vit.config.hidden_size\n",
    "        self.classifier = nn.Linear(combined_size, 1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_text = text_outputs.pooler_output  # [batch_size, hidden_size]\n",
    "        image_outputs = self.vit(pixel_values=images)\n",
    "        cls_image = image_outputs.last_hidden_state[:, 0]  # [batch_size, hidden_size]\n",
    "        combined = torch.cat((pooled_text, cls_image), dim=1)  # [batch_size, combined_size]\n",
    "        logits = self.classifier(combined)  # [batch_size, 1]\n",
    "        return logits\n",
    "\n",
    "def calculate_accuracy(logits, labels):\n",
    "    # Apply sigmoid to logits and threshold at 0.5\n",
    "    preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "    correct = (preds == labels).sum().item()\n",
    "    return correct / labels.size(0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # File paths for your datasets\n",
    "    train_file_path = 'crisismmd_datasplit_all/crisismmd_datasplit_all/task_informative_text_img_train.tsv'\n",
    "    dev_file_path = 'crisismmd_datasplit_all/crisismmd_datasplit_all/task_informative_text_img_dev.tsv'\n",
    "    test_file_path = 'crisismmd_datasplit_all/crisismmd_datasplit_all/task_informative_text_img_test.tsv'\n",
    "    \n",
    "    # Load datasets\n",
    "    train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "    dev_data = pd.read_csv(dev_file_path, sep='\\t')\n",
    "    test_data = pd.read_csv(test_file_path, sep='\\t')\n",
    "    \n",
    "    # Initialize tokenizer and feature extractor\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    \n",
    "    # Define image transform for ViT requirements\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = CrisisDataset(train_data, tokenizer, feature_extractor, transform=image_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    # Instantiate models\n",
    "    text_model = TextClassifier()\n",
    "    image_model = ImageClassifier()\n",
    "    multimodal_model = MultiModalClassifier()\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Define optimizers\n",
    "    text_optimizer = optim.Adam(text_model.parameters(), lr=2e-5)\n",
    "    image_optimizer = optim.Adam(image_model.parameters(), lr=2e-5)\n",
    "    multimodal_optimizer = optim.Adam(multimodal_model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Set models to training mode\n",
    "    text_model.train()\n",
    "    image_model.train()\n",
    "    multimodal_model.train()\n",
    "    \n",
    "    # Sample training loop for one epoch (here, we loop over one batch for demonstration)\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids']         # [batch_size, seq_len]\n",
    "        attention_mask = batch['attention_mask']\n",
    "        images = batch['image']                  # [batch_size, 3, 224, 224]\n",
    "        label_image = batch['label_image'].unsqueeze(1)  # [batch_size, 1]\n",
    "        label = batch['label'].unsqueeze(1)              # [batch_size, 1]\n",
    "        \n",
    "        # Zero gradients\n",
    "        text_optimizer.zero_grad()\n",
    "        image_optimizer.zero_grad()\n",
    "        multimodal_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass for text model (predicts label_image)\n",
    "        text_logits = text_model(input_ids, attention_mask)\n",
    "        text_loss = criterion(text_logits, label_image)\n",
    "        text_acc = calculate_accuracy(text_logits, label_image)\n",
    "        \n",
    "        # Forward pass for image model (predicts label_image)\n",
    "        image_logits = image_model(images)\n",
    "        image_loss = criterion(image_logits, label_image)\n",
    "        image_acc = calculate_accuracy(image_logits, label_image)\n",
    "        \n",
    "        # Forward pass for multimodal model (predicts label)\n",
    "        multimodal_logits = multimodal_model(input_ids, attention_mask, images)\n",
    "        multimodal_loss = criterion(multimodal_logits, label)\n",
    "        multimodal_acc = calculate_accuracy(multimodal_logits, label)\n",
    "        \n",
    "        # Backward passes and optimizer steps\n",
    "        text_loss.backward()\n",
    "        text_optimizer.step()\n",
    "        \n",
    "        image_loss.backward()\n",
    "        image_optimizer.step()\n",
    "        \n",
    "        multimodal_loss.backward()\n",
    "        multimodal_optimizer.step()\n",
    "        \n",
    "        # Print loss and accuracy for each model\n",
    "        print(\"Batch Metrics:\")\n",
    "        print(\"  Text Model    -> Loss: {:.4f} | Accuracy: {:.4f}\".format(text_loss.item(), text_acc))\n",
    "        print(\"  Image Model   -> Loss: {:.4f} | Accuracy: {:.4f}\".format(image_loss.item(), image_acc))\n",
    "        print(\"  Multimodal    -> Loss: {:.4f} | Accuracy: {:.4f}\".format(multimodal_loss.item(), multimodal_acc))\n",
    "        \n",
    "        # For demonstration, we process only one batch. Remove the break to run through all batches.\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
